---
title: 'Machine Learning Algorithms'
date: 2019-10-27
permalink: /posts/2019/10/blog-post-10/
tags:
  - Note
---

This is my personal notes for Machine Learning Algorithms.


Introduction
------
本文介绍各种经典的机器学习算法，包括随机森林，GBDT


Random Forest and GBDT
------
* Random Forest

Random Forest是经典的的bagging算法(减少variance)，由多棵决策树并行组成，每颗决策树既可以是分类树也可以是回归树。

* 生成过程:
  * (1) 每颗决策树用有放回抽样的方法从原始样本(m个)中抽取m个样本--所以每个决策树使用的样本基本都有重复
  * (2) 每颗决策树从m个总特征中随机抽取k个，用决策树的方法每次选择最优的特征进行分裂
  * (3) 循环生成若干个决策树，每次预测时每个决策树给出一个结果，所有决策树投票产生预测结果

Sample:

```python

RF=RandomForestRegressor(n_estimators=10,max_features=4,min_samples_split=30)
RF.fit(train_x,train_y)
Prediction3=RF.predict(test_x)
Prediction3=np.expm1(Prediction3)
print(r2_score(test_y,Prediction3))

```

* 参数说明
  * n_estimators: 决策树数量  
  * max_features: 每棵决策树抽取的feature数量上限  
  * min_samples_split: 内部节点再划分所需最小样本数，如果某节点样本量小于这个数，不再分裂  

* 模型优点:
  * (1) 实现简单，训练速度快，可以并行实现，因为训练时树与树之间是相互独立的；
  * (2) 相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；
  * (3) 能处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的；
  * (4) 对于不平衡的数据集，可以平衡误差，对异常值不敏感；
  * (5) 训练完成后可以给出哪些特征比较重要。


* GBDT:

GBDT是经典的的boosting算法(减少bias)，由多棵决策树串行组成，决策树只能是回归树。

* 生成过程:
  * (1) 算法每次迭代生成一颗新的决策树;
  * (2) 在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数和二阶导数;
  * (3) 通过贪心策略生成新的决策树，并计算每个叶节点对应的预测值;
  * (4) 把新生成的决策树添加到模型中，通常这个时候会加入一个步长(学习率)。

Sample:

```python

GBDT=GradientBoostingRegressor(n_estimators=30,learning_rate=0.1,loss='ls',subsample=1.0)
GBDT.fit(train_x,train_y)
Prediction1=GBDT.predict(test_x)
Prediction1=np.expm1(Prediction1)
print(r2_score(test_y,Prediction1))

```

* 参数说明
  * n_estimators: 串行树的个数，即迭代次数，树越多，准确率变高，但效率变差
  * learning_rate: GBDT在每增加一颗新树进来的时候，系统并不完全相信这颗树的拟合程度，而是只采纳了其中一部分(学习率), 步长(学习率)越长，准确率越高，但泛化能力越弱
  * loss: 损失函数，ls为均方差，回归时用，用来做分类是loss改为对数似然函数
  * subsample: 采样比例，选择一定比例的样本放入GBDT进行拟合(无放回)

和RF不同的是，GBDT从第二颗树开始，导入的就是残差，而不是样本，这就带来了一个问题。RF模型因为是导入样本，因此它可以轻松的建立最优的树结构，而GBDT导入的是残差，因此建立最优树结构比RF难多了。为解决这个问题，梯度下降出现了。大神Freidman提出了用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。之前提到的问题迎刃而解了。[GBDT算法推导](https://www.zybuluo.com/yxd/note/611571) 

* 模型优点:
  * 适用面广，离散或连续的数据都可以处理，几乎可用于所有回归问题（线性/非线性），亦可用于二分类问题。
* 模型缺点:
  * 由于弱分类器的串行依赖，难以并行训练数据。但预测时是并行的，预测结果是所有树的预测值之和。


* Random Forest 和 GBDT 的对比

  * 随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。
  * 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。
  * 组成随机森林的树可以并行生成；而GBDT只能是串行生成。
  * 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。
  * 随机森林对异常值不敏感；GBDT对异常值非常敏感。
  * 随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。
  * 随机森林是通过减少模型方差(variance)提高性能；GBDT是通过减少模型偏差(bias)提高性能。