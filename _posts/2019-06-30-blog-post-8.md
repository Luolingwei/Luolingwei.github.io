---
title: 'Elasticsearch'
date: 2019-06-30
permalink: /posts/2019/07/blog-post-8/
tags:
  - Note
---

This is my personal notes for elasticsearch.


Introduction
------
Elasticsearch是一个分布式的 RESTful 风格的搜索和数据分析引擎, Kibana是一个方便的ES插件  
python操作ES也十分方便, 导入es包，实例化es之后便可以进行各种操作  
 
------------------

ES存储方式
------
![avatar](https://upload-images.jianshu.io/upload_images/6468203-aafc8beae4638971.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

不同于MySQL和HBase, ES的存储方式是索引存储

关系数据库 ⇒ 数据库 ⇒ 表 ⇒ 行 ⇒ 列(Columns)  
Elasticsearch ⇒ 索引 ⇒ 类型 ⇒ 文档 ⇒ 字段(Fields)  
一个 Elasticsearch 集群可以包含多个索引(数据库)，其中包含了很多类型(表)。这些类型中包含了很多的文档(行)，然后每个文档中又包含了很多的字段(列)。每条记录都是一个文档, 用JSON序列化。

------------------

ES索引
------
Elasticsearch最关键的就是提供强大的索引能力了, Elasticsearch索引的精髓：一切设计都是为了提高搜索的性能, 另一层意思：为了提高搜索的性能，难免会牺牲某些其他方面，比如插入/更新。

往Elasticsearch里插入一条记录，其实就是直接PUT一个json的对象，这个对象有多个fields，比如name, sex, age, interests，那么在插入这些数据到Elasticsearch的同时，Elasticsearch还自动为这些字段建立索引–倒排索引，因为Elasticsearch最核心功能是搜索。

* 倒排索引:   

    传统的我们的检索是通过文章，逐个遍历找到对应关键词的位置。  
    而倒排索引，是通过分词策略，形成了词和文章的映射关系表，这种词典+映射表， 即为倒排索引。  
    有了倒排索引，就能实现 o（1）时间复杂度的效率检索文章了，极大的提高了检索效率。

    ![avatar](https://pic4.zhimg.com/80/v2-43542fcc0daf345b92c5a674c4197e8b_720w.jpg)

------------------

ES索引数据过多如何调优，部署
-----------------------

* (1) 动态索引层面

    基于模板+时间+rollover api 滚动创建索引，举例：设计阶段定义：blog 索引的模板格式为：blog_index_时间戳的形式，每天递增数据。  

    这样做的好处：不至于数据量激增导致单个索引数据量非常大，接近于上线 2 的32 次幂-1，索引存储达到了 TB+甚至更大。一旦单个索引很大，存储等各种风险也随之而来，所以要提前考虑+及早避免。

* (2) 存储层面

    冷热数据分离存储，热数据（比如最近 3 天或者一周的数据），其余为冷数据。

    对于冷数据不会再写入新数据，可以考虑定期 force_merge 加 shrink 压缩操作，节省存储空间和检索效率。

* (3) 部署层面

    一旦之前没有规划，这里就属于应急策略。

    结合 ES 自身的支持动态扩展的特点，动态新增机器的方式可以缓解集群压力
    
    注意：如果之前主节点等规划合理，不需要重启集群也能完成动态新增的。

------------------

在并发情况下，Elasticsearch如果保证读写一致
------

* 可以通过版本号使用乐观并发控制(OCC)，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；

* 对于写操作，一致性级别支持quorum/one/all，默认为quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。

* 对于读操作，可以设置replication为sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置replication为async时，也可以通过设置搜索请求参数_preference为primary来查询主分片，确保文档是最新版本。

* Note: 
    * OCC是用来解决**写-写冲突**的无锁并发控制，认为事务间争用没有那么多，所以先进行修改，在提交事务前，检查一下事务开始后，有没有新提交改变，如果没有就提交，如果有就放弃并重试。乐观并发控制类似自选锁。乐观并发控制适用于低数据争用，写冲突比较少的环境。

    * MVCC是用来解决**读-写冲突**的无锁并发控制，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 这样在读操作不用阻塞写操作，写操作不用阻塞读操作的同时，避免了脏读和不可重复读。

------------------

es写入数据的过程
--------------
* 1 客户端选择一个node发送请求过去，这个node就是coordinating node (协调节点)
* 2 coordinating node，对document进行路由，将请求转发给对应的node
* 3 实际上的node上的primary shard处理请求，然后将数据同步到replica node
* 4 coordinating node，如果发现primary node和所有的replica node都搞定之后，就会返回请求到客户端

------------------

es读数据过程
----------
查询，GET某一条的数据，写入某个document，这个document会自动给你分配一个全局的唯一ID，同时跟住这个ID进行hash路由到对应的primary shard上面去，当然也可以手动的设置ID

* 1 客户端发送任何一个请求到任意一个node，成为coordinate node
* 2 coordinate node 对document进行路由，将请求转发到对应的node，此时会使用round-robin随机轮训算法，在primary shard 以及所有的replica中随机选择一个，让读请求负载均衡，
* 3 接受请求的node，返回document给coordinate note
* 4 coordinate node返回给客户端

------------------

es搜索数据过程
------------
* 1 客户端发送一个请求给coordinate node
* 2 协调节点将搜索的请求转发给所有的shard对应的primary shard 或replica shard
* 3 query phase：每一个shard 将自己搜索的结果（其实也就是一些唯一标识），返回给协调节点，有协调节点进行数据的合并，排序，分页等操作，产出最后的结果
* 4 fetch phase ，接着由协调节点，根据唯一标识去各个节点进行拉去数据，最总返回给客户端

------------------

写入数据的底层原理
----------------
* 1 数据先写入到buffer里面，在buffer里面的数据时搜索不到的，同时将数据写入到translog日志文件之中
* 2 如果buffer快满了，或是一段时间之后，就会将buffer数据refresh到一个新的OS cache之中，然后每隔1秒，就会将OS cache的数据写入到segment file之中，但是如果每一秒钟没有新的数据到buffer之中，就会创建一个新的空的segment file，只要buffer中的数据被refresh到OS cache之中，就代表这个数据可以被搜索到了。当然可以通过restful api 和Java api，手动的执行一次refresh操作，就是手动的将buffer中的数据刷入到OS cache之中，让数据立马搜索到，只要数据被输入到OS cache之中，buffer的内容就会被清空了。同时进行的是，数据到shard之后，就会将数据写入到translog之中，每隔5秒将translog之中的数据持久化到磁盘之中
* 3 重复以上的操作，每次一条数据写入buffer，同时会写入一条日志到translog日志文件之中去，这个translog文件会不断的变大，当达到一定的程度之后，就会触发commit操作。
* 4 将一个commit point写入到磁盘文件，里面标识着这个commit point 对应的所有segment file
* 5 强行将OS cache 之中的数据都fsync到磁盘文件中去。
* 6 将现有的translog文件进行清空，然后在重新启动一个translog，此时commit就算是成功了，默认的是每隔30分钟进行一次commit，但是如果translog的文件过大，也会触发commit，整个commit过程就叫做一个flush操作，我们也可以通过ES API,手动执行flush操作，手动将OS cache 的数据fsync到磁盘上面去，记录一个commit point，清空translog文件
* 7 如果时删除操作，commit的时候会产生一个.del文件，里面讲某个doc标记为delete状态，那么搜索的时候，会根据.del文件的状态，就知道那个文件被删除了。
* 8 如果时更新操作，就是讲原来的doc标识为delete状态，然后重新写入一条数据即可。
* 9 buffer每次更新一次，就会产生一个segment file 文件，所以在默认情况之下，就会产生很多的segment file 文件，将会定期执行merge操作
* 10 每次merge的时候，就会将多个segment file 文件进行合并为一个，同时将标记为delete的文件进行删除，然后将新的segment file 文件写入到磁盘，这里会写一个commit point，标识所有的新的segment file，然后打开新的segment file供搜索使用。

总之，segment的四个核心概念，refresh，flush，translog、merge

------------------

搜索数据的底层原理
----------
查询过程大体上分为查询和取回这两个阶段，广播查询请求到所有相关分片，并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端。

* 1 查询阶段

  * a 当一个节点接收到一个搜索请求，这这个节点就会变成协调节点，第一步就是将广播请求到搜索的每一个节点的分片拷贝，查询请求可以被某一个主分片或某一个副分片处理，协调节点将在之后的请求中轮训所有的分片拷贝来分摊负载。
  * b 每一个分片将会在本地构建一个优先级队列，如果客户端要求返回结果排序中从from 名开始的数量为size的结果集，每一个节点都会产生一个from+size大小的结果集，因此优先级队列的大小也就是from+size，分片仅仅是返回一个轻量级的结果给协调节点，包括结果级中的每一个文档的ID和进行排序所需要的信息。
  * c 协调节点将会将所有的结果进行汇总，并进行全局排序，最总得到排序结果。

* 2 取值阶段

  * a 查询过程得到的排序结果，标记处哪些文档是符合要求的，此时仍然需要获取这些文档返回给客户端
  * b 协调节点会确定实际需要的返回的文档，并向含有该文档的分片发送get请求，分片获取的文档返回给协调节点，协调节点将结果返回给客户端。


------------------

实例展示
------

```python
from elasticsearch import Elasticsearch
es = Elasticsearch(['mbd-rec-elasticsearch-online006-bdwg.xxx.xxx:xxxx'], port=9200) #集群的一个节点

```

* ES查询

    example 1  

    "must"==and, 'should'==or, 'range'==between, 'exists'==非空, 'match_phrase'=='=='  

    ```json
        body2={
        "query": {
            "bool": {
            "must": [
                        {"range": { "tag_cnt": {"gte": "1","lt": None}}},
                        {"range": {"duration": {"gte": 61, "lt": 900}}},

                        {"exists": {"field": "father_id"}},

                        {"match_phrase": {
                            "access": {
                                "query": True
                                }
                            }
                        },


                        {"match_phrase": {
                            "soft_porn": {
                                "query": False
                            }
                        }
                        },

                        {
                            "bool": {
                                "should": [

                                    {"match_phrase": {
                                        "lchannel": {
                                            "query": 1
                                        }
                                    }
                                    },

                                    {"match_phrase": {
                                        "lchannel": {
                                            "query": 2
                                        }
                                    }
                                    },

                                    {"match_phrase": {
                                        "lchannel": {
                                            "query": 3
                                        }
                                    }
                                    },

                                    {"match_phrase": {
                                        "lchannel": {
                                            "query": 4
                                        }
                                    }
                                    },

                                    {"match_phrase": {
                                        "lchannel": {
                                            "query": 6
                                        }
                                    }
                                    },

                                    {"match_phrase": {
                                        "lchannel": {
                                            "query": 15
                                        }
                                    }
                                    },

                                    {"match_phrase": {
                                        "lchannel": {
                                            "query": 31
                                        }
                                    }
                                    }

                                ],
                                "minimum_should_match": 1

                            }
                        }

            ]
            }
        }
        }

        page2 = es.search(index='stv', doc_type='stv', body=body2)
        DS_short=page2['hits']['total']
        
    ```  

    example 2  

    这里增加了聚合操作'aggs'，相当于distinct count('father_id'), 统计符合条件的 father_id 数量  

    ```json

        body4={
            "aggs": {
                "by_fatherId": {
                    "cardinality": {
                        "field": "father_id"
                    }
                }
            },

            "query": {
            "bool": {
            "must": [
                        {"range": { "home_cps_time": {"gte": "now-7d/d","lt": "now"}}},
                        {"range": {"album": {"gte": 0, "lt": 1}}},
                        {"exists": {"field": "father_id"}},
            ]
            }
        }
        }

        page4 = es.search(index='stv', doc_type='stv', body=body4)
        corpus_long2=page4['aggregations']['by_fatherId']['value']

    ```

* ES写入

    1 es更新(update)操作, 这里需要id已经在es中存在，会覆盖原来的数据, body可传入各种形式的json数据

    ```python

        body = {"doc": {
            'lidCount_online': lidCount_online,
            'sidCount_online': sidCount_online,
            'lidCount_rate': lidCount_rate,
            'sidCount_rate': sidCount_rate
        }
    }

        es.update(index='corpus_daily_statistics', doc_type='st', id=date, body=body)

    ```  

    2 es插入(index)操作, 这里新建一个id为Today的es记录，body可传入各种形式的json数据    
    如 {'A':{'B':[{'C':'D'},{'E':'F'}]}}

    ```python

        es.index(index='core_indicator_daily_analysis', doc_type='st', id=Today, body=data)

    ```